{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convas_image.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMS6Xr4+rJh5ls42M4thG4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssifood/image_segmentation_U-NET/blob/main/convas_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIhuBLhcpyFw"
      },
      "source": [
        "#this code copied by  https://www.kaggle.com/dikshabhati2002/image-segmentation-u-net.  (kaggle code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTWvZt564e5H"
      },
      "source": [
        "Image Segmentation\n",
        "\n",
        "An image is a collection or set of different pixels. We group together the pixels that have similar attributes using image segmentation.. Thus, the task of image segmentation is to train a neural network to output a pixel-wise mask of the image. This helps in understanding the image at a much lower level, i.e., the pixel level.In image segmetation each pixel is given a label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caP55JPsmsnG"
      },
      "source": [
        "import libraries\n",
        "Before diving into code, first we will need to import all the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6iD5us2vPUQ"
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()    #kaggle.json 업로드 (kaggle api )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjmDJBIj3693"
      },
      "source": [
        "#https://soohee410.github.io/colab_kaggle 참고사이트 \n",
        "\n",
        "!mkdir -p ~/.kaggle #create folder name Kaggle\n",
        "!cp kaggle.json ~/.kaggle/ # copy kaggle.json into folder Kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQuiE-lkgYqV"
      },
      "source": [
        "!mkdir -p kaggle\n",
        "!cd /content/kaggle\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5M-hSjztID"
      },
      "source": [
        "!kaggle competitions download -c carvana-image-masking-challenge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgmVtsyBDIfq"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile \n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Minx-EHwmy4x"
      },
      "source": [
        "Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIoZzrdXm0-h"
      },
      "source": [
        "#train_zip = \"/kaggle/input/carvana-image-masking-challenge/train.zip\"\n",
        "train_zip = \"train.zip\"\n",
        "with ZipFile(train_zip, 'r') as zip_: \n",
        "    zip_.extractall('content/kaggle/working')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L79QI-z-m4GC"
      },
      "source": [
        "#train_mask_zip = \"/kaggle/input/carvana-image-masking-challenge/train_masks.zip\"\n",
        "train_mask_zip = \"train_masks.zip\"\n",
        "with ZipFile(train_mask_zip, 'r') as zip_: \n",
        "    zip_.extractall('/content/kaggle/working')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AKCqK-Im7xS"
      },
      "source": [
        "print(\"Train set:  \", len(os.listdir(\"/content/kaggle/working/train\")))\n",
        "print(\"Train masks:\", len(os.listdir(\"/content/kaggle/working/train_masks\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_Wz5nQm9OV"
      },
      "source": [
        "car_ids = []\n",
        "paths = []\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/working/train'):\n",
        "    for filename in filenames:\n",
        "        path = os.path.join(dirname, filename)    \n",
        "        paths.append(path)\n",
        "        \n",
        "        car_id = filename.split(\".\")[0]\n",
        "        car_ids.append(car_id)\n",
        "\n",
        "d = {\"id\": car_ids, \"car_path\": paths}\n",
        "df = pd.DataFrame(data = d)\n",
        "df = df.set_index('id')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y0em2wIm_cv"
      },
      "source": [
        "car_ids = []\n",
        "mask_path = []\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/working/train_masks'):\n",
        "    for filename in filenames:\n",
        "        path = os.path.join(dirname, filename)\n",
        "        mask_path.append(path)\n",
        "        \n",
        "        car_id = filename.split(\".\")[0]\n",
        "        car_id = car_id.split(\"_mask\")[0]\n",
        "        car_ids.append(car_id)\n",
        "\n",
        "        \n",
        "d = {\"id\": car_ids,\"mask_path\": mask_path}\n",
        "mask_df = pd.DataFrame(data = d)\n",
        "mask_df = mask_df.set_index('id')\n",
        "mask_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVctbM70nA3Y"
      },
      "source": [
        "df[\"mask_path\"] = mask_df[\"mask_path\"]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVQAdOQJuaD"
      },
      "source": [
        "Now we will perform a simple augmentation of flipping an image and then normalize the image pixel in between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHW9x960nC-G"
      },
      "source": [
        "img_size = [256,256]\n",
        "\n",
        "def data_augmentation(car_img, mask_img):\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        car_img = tf.image.flip_left_right(car_img)\n",
        "        mask_img = tf.image.flip_left_right(mask_img)\n",
        "\n",
        "    return car_img, mask_img\n",
        "\n",
        "def preprocessing(car_path, mask_path):\n",
        "    car_img = tf.io.read_file(car_path) \n",
        "    car_img = tf.image.decode_jpeg(car_img, channels=3)\n",
        "    car_img = tf.image.resize(car_img, img_size)\n",
        "    car_img = tf.cast(car_img, tf.float32) / 255.0\n",
        "    \n",
        "    mask_img = tf.io.read_file(mask_path)\n",
        "    mask_img = tf.image.decode_jpeg(mask_img, channels=3)\n",
        "    mask_img = tf.image.resize(mask_img, img_size)\n",
        "    mask_img = mask_img[:,:,:1]    \n",
        "    mask_img = tf.math.sign(mask_img)\n",
        "    \n",
        "    \n",
        "    return car_img, mask_img\n",
        "\n",
        "def create_dataset(df, train = False):\n",
        "    if not train:\n",
        "        ds = tf.data.Dataset.from_tensor_slices((df[\"car_path\"].values, df[\"mask_path\"].values))\n",
        "        ds = ds.map(preprocessing, tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_tensor_slices((df[\"car_path\"].values, df[\"mask_path\"].values))\n",
        "        ds = ds.map(preprocessing, tf.data.AUTOTUNE)\n",
        "        ds = ds.map(data_augmentation, tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MbTGfALnC7l"
      },
      "source": [
        "train_df, valid_df = train_test_split(df, random_state=42, test_size=.25)\n",
        "train = create_dataset(train_df, train = True)\n",
        "valid = create_dataset(valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR4TS6CsnC0N"
      },
      "source": [
        "TRAIN_LENGTH = len(train_df)\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7aKKPICnCrr"
      },
      "source": [
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmf_hOiJzsJ"
      },
      "source": [
        "Let's look the image and it's corresponding mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uoF4DgtnHr9"
      },
      "source": [
        "def display(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbal_UptnHpA"
      },
      "source": [
        "for i in range(5):\n",
        "   for image, mask in train.take(i):\n",
        "        sample_image, sample_mask = image, mask\n",
        "        display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QNmekJnHmc"
      },
      "source": [
        "Model\n",
        "We are going to use U-Net model. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder.The encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in tf.keras.applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU-2DKlhnHfS"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "down_stack.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhBCNhmnN7H"
      },
      "source": [
        "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    \n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    if norm_type.lower() == 'batchnorm':\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "    elif norm_type.lower() == 'instancenorm':\n",
        "        result.add(InstanceNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "        result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr0KoSArnN42"
      },
      "source": [
        "def unet_model(output_channels):\n",
        "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2, activation='sigmoid',\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F5H0H6onN2V"
      },
      "source": [
        "Train the Model\n",
        "Now let's compile the model and see the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlGlbFoCnNz_"
      },
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "def dice_loss(in_gt, in_pred):\n",
        "    return 1-dice_coef(in_gt, in_pred)\n",
        "\n",
        "model = unet_model(1)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = dice_loss,\n",
        "              metrics=[dice_coef,'binary_accuracy'])\n",
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOieGsKZJ9pL"
      },
      "source": [
        "Let's try out the model to see what it predicts before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBl5kbqInNxU"
      },
      "source": [
        "for images, masks in train_dataset.take(1):\n",
        "    for img, mask in zip(images, masks):\n",
        "        sample_image = img\n",
        "        sample_mask = mask\n",
        "        break\n",
        "def visualize(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_predictions(sample_image, sample_mask):\n",
        "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
        "    pred_mask = pred_mask.reshape(img_size[0],img_size[1],1)\n",
        "    visualize([sample_image, sample_mask, pred_mask])\n",
        "    \n",
        "show_predictions(sample_image, sample_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQCv4NBnNvH"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh1Vm0w-KAl5"
      },
      "source": [
        "Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5TUDiRynNsV"
      },
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=4,restore_best_weights=True)\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            show_predictions(sample_image, sample_mask)\n",
        "EPOCHS = 15\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_data=valid_dataset,\n",
        "                          callbacks=[DisplayCallback(), early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr3vIyJDnNpw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}